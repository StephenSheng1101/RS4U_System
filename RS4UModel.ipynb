{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPoEgpu5aTqcFb5cg9qJP24",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StephenSheng1101/RS4U_System/blob/main/RS4UModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7NTgjeveK0g"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification, AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "import tf2onnx\n",
        "import onnx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Yelp dataset (replace 'path_to_yelp_dataset.csv' with the actual path to your Yelp dataset file)\n",
        "filename = 'yelp_review.csv'"
      ],
      "metadata": {
        "id": "cnGK69aCicdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read CSV file\n",
        "df = pd.read_csv(filename, encoding='utf-8', on_bad_lines=\"skip\", engine=\"python\")\n",
        "# Limit the dataset size to 1000 rows\n",
        "df = df.head(10000)\n",
        "\n",
        "# Assuming your dataset has 'stars' as the rating and 'text' as the review text\n",
        "data = {'text': df['text'].values, 'stars': df['stars'].values}\n",
        "\n",
        "# Map star ratings to sentiment classes\n",
        "data['sentiment'] = pd.cut(data['stars'], bins=[0, 2, 3, 5], labels=['negative', 'neutral', 'positive'])\n",
        "\n",
        "# Convert the dictionary to a Pandas DataFrame\n",
        "df_data = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "d4aWAm9RijBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training, validation, and test sets\n",
        "train_data, test_data = train_test_split(df_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# If you want to further split for validation, you can do the following\n",
        "valid_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n"
      ],
      "metadata": {
        "id": "Mv2avxPbikfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT tokenizer and model (using bert-base-cased)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=3)  # 3 classes: negative, neutral, positive\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oLIoVKgiksL",
        "outputId": "675a9236-b005-40e7-d3a2-2ee81f5828fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom dataset\n",
        "class CustomDataset(tf.keras.utils.Sequence):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128, batch_size=8):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.batch_size = batch_size\n",
        "        self.label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
        "\n",
        "    #def __len__(self):\n",
        "        #return len(self.texts) // self.batch_size\n",
        "    def __len__(self):\n",
        "        return (len(self.texts) + self.batch_size - 1) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_texts = self.texts[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
        "        batch_labels = self.labels[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
        "\n",
        "        # Convert string labels to numerical values\n",
        "        batch_labels = [self.label_mapping[label] for label in batch_labels]\n",
        "\n",
        "        # Tokenize the batch of texts\n",
        "        tokens = self.tokenizer.batch_encode_plus(\n",
        "            batch_texts,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='tf'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': tokens['input_ids'],\n",
        "            'attention_mask': tokens['attention_mask'],\n",
        "            'label': tf.convert_to_tensor(batch_labels, dtype=tf.int32)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "ukETYDyIisFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and create DataLoader\n",
        "def create_dataloader(data, tokenizer, max_length=128, batch_size=8):\n",
        "    dataset = CustomDataset(texts=data['text'], labels=data['sentiment'], tokenizer=tokenizer, max_length=max_length, batch_size=batch_size)\n",
        "    dataloader = tf.data.Dataset.from_generator(lambda: dataset, output_signature={\n",
        "        'input_ids': tf.TensorSpec(shape=(None, max_length), dtype=tf.int32),\n",
        "        'attention_mask': tf.TensorSpec(shape=(None, max_length), dtype=tf.int32),\n",
        "        'label': tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
        "    })\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "xZhVM_67isHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 128\n",
        "batch_size = 8\n",
        "train_dataloader = create_dataloader(train_data, tokenizer, max_length=max_length, batch_size=batch_size)\n",
        "valid_dataloader = create_dataloader(valid_data, tokenizer, max_length=max_length, batch_size=batch_size)\n",
        "test_dataloader = create_dataloader(test_data, tokenizer, max_length=max_length, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "SQYKIj2UisKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "criterion = SparseCategoricalCrossentropy(from_logits=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzv5fB_risL6",
        "outputId": "c4cd084e-998f-4a95-f7b0-1d34f90df875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    num_batches = tf.data.experimental.cardinality(train_dataloader).numpy()\n",
        "    #num_batches = len(train_dataloader)\n",
        "\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        inputs = {'input_ids': batch['input_ids'], 'attention_mask': batch['attention_mask']}\n",
        "        labels = batch['label']\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            outputs = model(inputs, training=True)\n",
        "            loss = criterion(labels, outputs.logits)\n",
        "\n",
        "        total_loss += loss.numpy()\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    # Calculate average training loss\n",
        "    avg_train_loss = total_loss / num_batches\n",
        "\n",
        "    # Validation\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for batch in valid_dataloader:\n",
        "        inputs = {'input_ids': batch['input_ids'], 'attention_mask': batch['attention_mask']}\n",
        "        labels = batch['label']\n",
        "\n",
        "        outputs = model(inputs, training=False)\n",
        "        preds = tf.argmax(outputs.logits, axis=1)\n",
        "        all_preds.extend(preds.numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "    # Calculate accuracy on validation set\n",
        "    accuracy_valid = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Avg Train Loss: {avg_train_loss:.4f}, Validation Accuracy: {accuracy_valid:.4f}')\n"
      ],
      "metadata": {
        "id": "dNVsyQYKisO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing on the original model trained using the test set\n",
        "all_preds_test = []\n",
        "all_labels_test = []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    inputs = {'input_ids': batch['input_ids'], 'attention_mask': batch['attention_mask']}\n",
        "    labels = batch['label']\n",
        "\n",
        "    outputs = model(inputs, training=False)\n",
        "    preds = tf.argmax(outputs.logits, axis=1)\n",
        "    all_preds_test.extend(preds.numpy())\n",
        "    all_labels_test.extend(labels.numpy())\n",
        "\n",
        "# Calculate accuracy, precision, recall, and f1 score on the test set\n",
        "accuracy_test = accuracy_score(all_labels_test, all_preds_test)\n",
        "precision = precision_score(all_labels_test, all_preds_test, average='weighted')\n",
        "recall = recall_score(all_labels_test, all_preds_test, average='weighted')\n",
        "f1 = f1_score(all_labels_test, all_preds_test, average='weighted')\n",
        "print(f'Accuracy (Original Model): {accuracy_test:.4f}')\n",
        "print(f'Precision (Original Model): {precision:.4f}')\n",
        "print(f'Recall (Original Model): {recall:.4f}')\n",
        "print(f'F1 Score (Original Model): {f1:.4f}')\n",
        "\n",
        "# Confusion matrix on the test set\n",
        "conf_matrix = confusion_matrix(all_labels_test, all_preds_test)\n",
        "print('Confusion Matrix (Original Model):')\n",
        "print('               Predicted Positive Predicted Negative')\n",
        "print(f'Actual Positive      {conf_matrix[0, 0]}                 {conf_matrix[0, 1]}')\n",
        "print(f'Actual Negative      {conf_matrix[1, 0]}                 {conf_matrix[1, 1]}')\n"
      ],
      "metadata": {
        "id": "5LmJNyRiisQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "#model.save('C:/FYP/RS4U_Model/tf_model')\n",
        "model.save_pretrained('colab_tf_model')"
      ],
      "metadata": {
        "id": "tVdMxuM-isUm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/StephenSheng1101/RS4U_System/blob/main/RS4UModel.ipynb",
      "authorship_tag": "ABX9TyMaXi1f6ljia8rLRh0E4PY5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StephenSheng1101/RS4U_System/blob/main/RS4UModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7NTgjeveK0g"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification, AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "import tf2onnx\n",
        "import onnx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Q1EfqIrbK3RH",
        "outputId": "5a2db919-9c46-4736-c977-c4730ed33ffe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Yelp dataset (replace 'path_to_yelp_dataset.csv' with the actual path to your Yelp dataset file)\n",
        "filename = '/content/drive/MyDrive/Dataset/yelp_review.csv'"
      ],
      "metadata": {
        "id": "cnGK69aCicdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read CSV file\n",
        "df = pd.read_csv(filename, encoding='utf-8', on_bad_lines=\"skip\", engine=\"python\")\n",
        "# Limit the dataset size to 1000 rows"
      ],
      "metadata": {
        "id": "d4aWAm9RijBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nInformation of Dataset\")\n",
        "print(df.info())\n",
        "\n",
        "# Understanding the dataset's structure and dimensions\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"Column names:\", df.columns)\n",
        "\n",
        "# Data Understanding\n",
        "print(\"Print Data\")\n",
        "print(df[[\"review_id\", \"user_id\", \"business_id\"]].head())\n",
        "print(df[[\"date\", \"stars\", \"text\"]].head())\n",
        "print(df[[\"useful\", \"funny\", \"cool\"]].head())"
      ],
      "metadata": {
        "id": "x4mozfvTPZjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Limit the dataset size\n",
        "df = df.head(1000000) #1000000 50000\n",
        "\n",
        "# Assuming your dataset has 'stars' as the rating and 'text' as the review text\n",
        "data = {'text': df['text'3444].values, 'stars': df['stars'].values}\n",
        "\n",
        "# Map star ratings to sentiment classes\n",
        "data['sentiment'] = pd.cut(data['stars'], bins=[0, 2, 3, 5], labels=['negative', 'neutral', 'positive'])\n",
        "\n",
        "# Convert the dictionary to a Pandas DataFrame\n",
        "df_data = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "nD_zhbt5Pe8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training, validation, and test sets\n",
        "train_data, test_data = train_test_split(df_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# If you want to further split for validation, you can do the following\n",
        "valid_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "Mv2avxPbikfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT tokenizer and model (using bert-base-cased)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=3)  # 3 classes: negative, neutral, positive\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oLIoVKgiksL",
        "outputId": "675a9236-b005-40e7-d3a2-2ee81f5828fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save original tokenizer configuration and vocabulary\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/Dataset/RS4U_Model/original_tokenizer')\n",
        "original_vocab = tokenizer.save_vocabulary('/content/drive/MyDrive/Dataset/RS4U_Model/original_vocabulary')\n"
      ],
      "metadata": {
        "id": "UM46L-raPprR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom dataset\n",
        "class CustomDataset(tf.keras.utils.Sequence):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128, batch_size=8):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.batch_size = batch_size\n",
        "        self.label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
        "\n",
        "    #def __len__(self):\n",
        "        #return len(self.texts) // self.batch_size\n",
        "    def __len__(self):\n",
        "        return (len(self.texts) + self.batch_size - 1) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_texts = self.texts[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
        "        batch_labels = self.labels[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
        "\n",
        "        # Convert string labels to numerical values\n",
        "        batch_labels = [self.label_mapping[label] for label in batch_labels]\n",
        "\n",
        "        # Tokenize the batch of texts\n",
        "        tokens = self.tokenizer.batch_encode_plus(\n",
        "            batch_texts,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='tf'\n",
        "        )\n",
        "        input_ids = tf.ensure_shape(tokens['input_ids'], (None, self.max_length))\n",
        "        attention_mask = tf.ensure_shape(tokens['attention_mask'], (None, self.max_length))\n",
        "\n",
        "        return {\n",
        "            'input_ids': tokens['input_ids'],\n",
        "            'attention_mask': tokens['attention_mask'],\n",
        "            'label': tf.convert_to_tensor(batch_labels, dtype=tf.int32)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "ukETYDyIisFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and create DataLoader\n",
        "def create_dataloader(data, tokenizer, max_length=128, batch_size=8):\n",
        "    dataset = CustomDataset(texts=data['text'], labels=data['sentiment'], tokenizer=tokenizer, max_length=max_length, batch_size=batch_size)\n",
        "    dataloader = tf.data.Dataset.from_generator(lambda: dataset, output_signature={\n",
        "        'input_ids': tf.TensorSpec(shape=(None, max_length), dtype=tf.int32),\n",
        "        'attention_mask': tf.TensorSpec(shape=(None, max_length), dtype=tf.int32),\n",
        "        'label': tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
        "    })\n",
        "    return dataloader\n"
      ],
      "metadata": {
        "id": "xZhVM_67isHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 128\n",
        "batch_size = 8\n",
        "train_dataloader = create_dataloader(train_data, tokenizer, max_length=max_length, batch_size=batch_size)\n",
        "valid_dataloader = create_dataloader(valid_data, tokenizer, max_length=max_length, batch_size=batch_size)\n",
        "test_dataloader = create_dataloader(test_data, tokenizer, max_length=max_length, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "SQYKIj2UisKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "criterion = SparseCategoricalCrossentropy(from_logits=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzv5fB_risL6",
        "outputId": "c4cd084e-998f-4a95-f7b0-1d34f90df875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 4\n",
        "for epoch in range(num_epochs):\n",
        "    total_train_loss = 0\n",
        "    total_val_loss = 0\n",
        "    num_batches = tf.data.experimental.cardinality(train_dataloader).numpy()\n",
        "    num_batches_val = tf.data.experimental.cardinality(valid_dataloader).numpy() #add\n",
        "    #num_batches = len(train_dataloader)\n",
        "\n",
        "    # Train\n",
        "    all_train_preds = []\n",
        "    all_train_labels = []\n",
        "    for batch in train_dataloader:\n",
        "        inputs = {'input_ids': batch['input_ids'], 'attention_mask': batch['attention_mask']}\n",
        "        labels = batch['label']\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            outputs = model(inputs, training=True)\n",
        "            loss = criterion(labels, outputs.logits)\n",
        "\n",
        "        total_train_loss += loss.numpy()\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "        preds = tf.argmax(outputs.logits, axis=1)\n",
        "        all_train_preds.extend(preds.numpy())\n",
        "        all_train_labels.extend(labels.numpy())\n",
        "\n",
        "    # Calculate average training loss and accuracy on train set\n",
        "    avg_train_loss = total_train_loss / num_batches\n",
        "    accuracy_train = accuracy_score(all_train_labels, all_train_preds)\n",
        "\n",
        "    # Validation\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for batch in valid_dataloader:\n",
        "        inputs = {'input_ids': batch['input_ids'], 'attention_mask': batch['attention_mask']}\n",
        "        labels = batch['label']\n",
        "\n",
        "        outputs = model(inputs, training=False)\n",
        "        loss = criterion(labels, outputs.logits) #add\n",
        "        total_val_loss += loss.numpy() #add\n",
        "        preds = tf.argmax(outputs.logits, axis=1)\n",
        "        all_preds.extend(preds.numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "    # Calculate average validation loss and accuracy on validation set\n",
        "    avg_val_loss = total_val_loss / num_batches_val\n",
        "    accuracy_valid = accuracy_score(all_labels, all_preds)\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Avg Train Loss: {avg_train_loss:.4f}, Train Accuracy: {accuracy_train:.4f}')\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Avg Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {accuracy_valid:.4f}')\n",
        "    #print(f'Epoch {epoch + 1}/{num_epochs}, Avg Train Loss: {avg_train_loss:.4f}, Avg Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {accuracy_valid:.4f}')\n",
        "    #print(f'Epoch {epoch + 1}/{num_epochs}, Avg Train Loss: {avg_train_loss:.4f}, Validation Accuracy: {accuracy_valid:.4f}')\n"
      ],
      "metadata": {
        "id": "dNVsyQYKisO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing on the original model trained using the test set\n",
        "all_preds_test = []\n",
        "all_labels_test = []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    inputs = {'input_ids': batch['input_ids'], 'attention_mask': batch['attention_mask']}\n",
        "    labels = batch['label']\n",
        "\n",
        "    outputs = model(inputs, training=False)\n",
        "    preds = tf.argmax(outputs.logits, axis=1)\n",
        "    all_preds_test.extend(preds.numpy())\n",
        "    all_labels_test.extend(labels.numpy())\n",
        "\n",
        "# Calculate accuracy, precision, recall, and f1 score on the test set\n",
        "accuracy_test = accuracy_score(all_labels_test, all_preds_test)\n",
        "precision = precision_score(all_labels_test, all_preds_test, average='weighted')\n",
        "recall = recall_score(all_labels_test, all_preds_test, average='weighted')\n",
        "f1 = f1_score(all_labels_test, all_preds_test, average='weighted')\n",
        "print(f'Accuracy (Original Model): {accuracy_test:.4f}')\n",
        "print(f'Precision (Original Model): {precision:.4f}')\n",
        "print(f'Recall (Original Model): {recall:.4f}')\n",
        "print(f'F1 Score (Original Model): {f1:.4f}')\n",
        "\n",
        "# Confusion matrix on the test set\n",
        "conf_matrix = confusion_matrix(all_labels_test, all_preds_test)\n",
        "print('Confusion Matrix (Original Model):')\n",
        "print('               Predicted Positive Predicted Negative')\n",
        "print(f'Actual Positive      {conf_matrix[0, 0]}                 {conf_matrix[0, 1]}')\n",
        "print(f'Actual Negative      {conf_matrix[1, 0]}                 {conf_matrix[1, 1]}')\n",
        "\n",
        "# Save the results to a text file\n",
        "results_file = '/content/drive/MyDrive/Dataset/results_original.txt'\n",
        "with open(results_file, 'w') as file:\n",
        "    file.write(f'Test Accuracy (Original Model): {accuracy_test:.4f}\\n')\n",
        "    file.write(f'Precision (Original Model): {precision:.4f}\\n')\n",
        "    file.write(f'Recall (Original Model): {recall:.4f}\\n')\n",
        "    file.write(f'F1 Score (Original Model): {f1:.4f}\\n')\n",
        "    file.write('Confusion Matrix (Original Model):\\n')\n",
        "    file.write('               Predicted Positive Predicted Negative\\n')\n",
        "    file.write(f'Actual Positive      {conf_matrix[0, 0]}                 {conf_matrix[0, 1]}\\n')\n",
        "    file.write(f'Actual Negative      {conf_matrix[1, 0]}                 {conf_matrix[1, 1]}\\n')\n"
      ],
      "metadata": {
        "id": "5LmJNyRiisQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "#model.save('C:/FYP/RS4U_Model/tf_model')\n",
        "model.save_pretrained('/content/drive/MyDrive/Dataset/tf_model')\n"
      ],
      "metadata": {
        "id": "tVdMxuM-isUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the tokenizer associated with the fine-tuned model\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/Dataset/rs4u_tokenizer')\n",
        "rs4u_vocab = tokenizer.save_vocabulary('/content/drive/MyDrive/Dataset/rs4u_vocabulary')\n"
      ],
      "metadata": {
        "id": "pNZMG-mHP9Cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model and tokenizer\n",
        "loaded_model = TFBertForSequenceClassification.from_pretrained('bert-base-cased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n"
      ],
      "metadata": {
        "id": "jmeI10_oP9G6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the TensorFlow model to ONNX format\n",
        "dummy_input = {\n",
        "    'input_ids': tf.zeros((1, 128), dtype=tf.int32),\n",
        "    'attention_mask': tf.zeros((1, 128), dtype=tf.int32),\n",
        "}\n",
        "\n",
        "# Define the input signature as a list of TensorSpec\n",
        "input_signature = [\n",
        "    tf.TensorSpec(shape=(None, 128), dtype=tf.int32, name='input_ids'),\n",
        "    tf.TensorSpec(shape=(None, 128), dtype=tf.int32, name='attention_mask'),\n",
        "]\n",
        "\n",
        "onnx_model, _ = tf2onnx.convert.from_keras(\n",
        "    loaded_model,\n",
        "    input_signature=input_signature,  # Pass the input signature here\n",
        ")\n",
        "\n",
        "print(\"save the onnx model\")\n",
        "# Save the ONNX model\n",
        "onnx.save_model(onnx_model, '/content/drive/MyDrive/Dataset/rs4u_model.onnx')\n",
        "\n",
        "print(\"Save as tensorflow life model\")\n",
        "# Convert the ONNX model to TensorFlow Lite with quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "with open('/content/drive/MyDrive/Dataset/rs4u_model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "id": "x66qYhw0P9JW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}